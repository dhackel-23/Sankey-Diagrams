{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "import pandas as pd\n",
    "from floweaver import *\n",
    "from mip import *\n",
    "from functools import cmp_to_key\n",
    "from attr import evolve\n",
    "import statistics\n",
    "import time\n",
    "from ipysankeywidget import SankeyWidget\n",
    "from ipywidgets import Layout, Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with Dataset.from_csv\n",
    "dataset = Dataset.from_csv(\"test_flows.csv\", \"test_processes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the partitions for the processes\n",
    "partition_farms = Partition.Simple('process', ['farm1', 'farm2', 'farm3'])\n",
    "partition_eat = Partition.Simple('process', ['domestic', 'industry'])\n",
    "partition_dest = Partition.Simple('process', ['compost', 'landfill'])\n",
    "partition_fruit = Partition.Simple('material', ['apples', 'bananas', 'oranges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the nodes, ordering and bundles \n",
    "nodes = {\n",
    "    'farms':    ProcessGroup('function == \"farms\"', partition=partition_farms, title='Farms'),\n",
    "    'eat':      ProcessGroup('function == \"consumers\"', partition=partition_eat, title='Eat'),\n",
    "    'end':      ProcessGroup('function == \"destination\"', partition=partition_dest, title='Destination'),\n",
    "    'fruit':    Waypoint(partition_fruit, title='fruit type')\n",
    "}\n",
    "\n",
    "ordering = [\n",
    "    [[], ['farms'], []],\n",
    "    [[], ['fruit'], []],\n",
    "    [[], ['eat'], []],\n",
    "    [[], ['end'], []],\n",
    "]\n",
    "\n",
    "bundles = [\n",
    "    Bundle('farms', 'eat', waypoints=['fruit']),\n",
    "    Bundle('eat', 'end'),\n",
    "    Bundle('farms', Elsewhere),\n",
    "    Bundle('eat', Elsewhere),\n",
    "    Bundle(Elsewhere,'eat'),\n",
    "    Bundle(Elsewhere,'end'),\n",
    "]\n",
    "\n",
    "# Color palette\n",
    "palette = {\n",
    "    'oranges': 'orange',\n",
    "    'bananas': 'yellow',\n",
    "    'apples': 'green',\n",
    "    '*': 'lightblue'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the SankeyDefinition\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering, flow_partition=dataset.partition('material'))\n",
    "\n",
    "# 2. Generate SankeyData (no optimisation)\n",
    "sankey_data = weave(sdd, dataset, palette=palette)\n",
    "\n",
    "# 3. Plot\n",
    "sankey_data.to_widget(width=700, height=450, margins=dict(left=70, right=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting new dataset optimised node ordering sankey diagram\n",
    "\n",
    "\n",
    "sankey_data_evolved = optimise_node_order(sankey_data, group_nodes=True)\n",
    "sankey_data_evolved.to_widget(width=700, height=450, margins=dict(left=100, right=120), debugging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_data_evolved.to_widget(layout=optimise_node_positions(sankey_data_evolved, scale = 10,minimum_gap=10, margins=dict(left=100, right=120)), width=700, height=450)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the SankeyDefinition\n",
    "\n",
    "new_flows = dataset._flows.copy()\n",
    "i_delete = 5\n",
    "row1 = new_flows.iloc[i_delete].copy()\n",
    "row1.update({'source': 'HIDDEN'})\n",
    "row2 = new_flows.iloc[i_delete].copy()\n",
    "row2.update({'target': 'HIDDEN'})\n",
    "new_flows = pd.concat([\n",
    "    new_flows.iloc[:i_delete],\n",
    "    new_flows.iloc[i_delete + 1:],\n",
    "    pd.DataFrame([row1, row2]),\n",
    "], ignore_index=True)\n",
    "#new_flows.loc[new_flows.source == 'eat1', 'source'] = 'HIDDEN'\n",
    "#new_flows.loc[new_flows.target == 'eat1', 'target'] = 'HIDDEN'\n",
    "\n",
    "new_dataset = Dataset(new_flows, dataset._dim_process, dataset._dim_material)\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering, flow_partition=new_dataset.partition('material'))\n",
    "\n",
    "# 2. Generate SankeyData (no optimisation)\n",
    "sankey_data = weave(sdd, new_dataset, palette=palette)\n",
    "\n",
    "# 3. Plot\n",
    "sankey_data.to_widget(width=700, height=450, margins=dict(left=70, right=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crossing_score_from_dict(crossing_areas):\n",
    "    crossing_count = len(crossing_areas)\n",
    "    crossing_area = sum(crossing_areas.values())\n",
    "    return {\n",
    "        \"crossing_count\": crossing_count,              # âœ… renamed\n",
    "        \"crossing_area\": crossing_area,                # âœ… renamed\n",
    "        \"crossing_score\": crossing_count * crossing_area\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_vertical_score_final(flows_df, y_coords, sankey_definition):\n",
    "    scores = []\n",
    "\n",
    "    # Detect waypoint layers\n",
    "    waypoint_groups = [key for key, val in sankey_definition.nodes.items() if \"Waypoint\" in str(type(val))]\n",
    "    layers = sankey_definition.ordering.layers\n",
    "    flat_ordering = [group[1][0] if group[1] else None for group in layers]\n",
    "\n",
    "    # Identify waypoint structures (left â†’ waypoint â†’ right)\n",
    "    waypoint_structures = {}\n",
    "    for idx, node in enumerate(flat_ordering):\n",
    "        if node in waypoint_groups and 0 < idx < len(flat_ordering) - 1:\n",
    "            wp = node\n",
    "            left = flat_ordering[idx - 1]\n",
    "            right = flat_ordering[idx + 1]\n",
    "            waypoint_structures[(left, right)] = wp\n",
    "\n",
    "    # Build node-to-group mapping (e.g., farm3 â†’ farms)\n",
    "    node_to_group = {}\n",
    "    for group_name, group_def in sankey_definition.nodes.items():\n",
    "        if hasattr(group_def, \"partition\"):\n",
    "            for subgroup in group_def.partition.groups:\n",
    "                node_to_group[subgroup.label] = group_name\n",
    "\n",
    "    for _, row in flows_df.iterrows():\n",
    "        src = row[\"source\"]\n",
    "        tgt = row[\"target\"]\n",
    "        width = row[\"value\"]\n",
    "        material = row[\"material\"]\n",
    "\n",
    "        src_group = node_to_group.get(src, \"\")\n",
    "        tgt_group = node_to_group.get(tgt, \"\")\n",
    "\n",
    "        score = None\n",
    "        span = None\n",
    "\n",
    "        # Use material name as the waypoint node\n",
    "        for (left, right), wp_group in waypoint_structures.items():\n",
    "            if src_group == left and tgt_group == right:\n",
    "                waypoint_node = material  # e.g., 'apples', 'bananas'\n",
    "                if src in y_coords and tgt in y_coords and waypoint_node in y_coords:\n",
    "                    dy1 = abs(y_coords[src] - y_coords[waypoint_node])\n",
    "                    dy2 = abs(y_coords[waypoint_node] - y_coords[tgt])\n",
    "                    score = (dy1 + dy2) / width if width > 0 else float(\"inf\")\n",
    "                    span = dy1 + dy2\n",
    "                    break\n",
    "\n",
    "        # Fallback to simple span\n",
    "        if score is None and src in y_coords and tgt in y_coords:\n",
    "            span = abs(y_coords[src] - y_coords[tgt])\n",
    "            score = span / width if width > 0 else float(\"inf\")\n",
    "\n",
    "        if score is not None:\n",
    "            scores.append({\n",
    "                \"source\": src,\n",
    "                \"target\": tgt,\n",
    "                \"material\": material,\n",
    "                \"width\": width,\n",
    "                \"span\": span,\n",
    "                \"vertical_score\": score\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(df, columns_to_normalize):\n",
    "    \"\"\"\n",
    "    Normalize specified columns in a DataFrame using min-max scaling.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with raw scores.\n",
    "        columns_to_normalize (list of str): Column names to normalize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized columns added (suffix \"_norm\").\n",
    "    \"\"\"\n",
    "    norm_df = df.copy()\n",
    "    for col in columns_to_normalize:\n",
    "        min_val = norm_df[col].min()\n",
    "        max_val = norm_df[col].max()\n",
    "        if max_val - min_val == 0:\n",
    "            # Avoid divide-by-zero if all values are the same\n",
    "            norm_df[f\"{col}_norm\"] = 0\n",
    "        else:\n",
    "            norm_df[f\"{col}_norm\"] = (norm_df[col] - min_val) / (max_val - min_val)\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_flows_by_index(dataset, row_indices):\n",
    "    \n",
    "    # Step 1: Copy original flows table\n",
    "    new_flows = dataset._flows.copy()\n",
    "    rows_to_hide = new_flows.iloc[row_indices]\n",
    "\n",
    "    # Step 2: Replace each flow with 2 hidden ones (source and target)\n",
    "    hidden_rows = []\n",
    "    for _, row in rows_to_hide.iterrows():\n",
    "        row_source = row.copy()\n",
    "        row_source[\"source\"] = \"HIDDEN\"\n",
    "\n",
    "        row_target = row.copy()\n",
    "        row_target[\"target\"] = \"HIDDEN\"\n",
    "\n",
    "        hidden_rows.extend([row_source, row_target])\n",
    "\n",
    "    # Step 3: Remove original and append hidden\n",
    "    new_flows = pd.concat([\n",
    "        new_flows.drop(index=row_indices),\n",
    "        pd.DataFrame(hidden_rows)\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    return new_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_flows_by_width_band(flows_df, target_frac=0.25, tolerance=0.10):\n",
    "    \"\"\"\n",
    "    Select a set of flows whose combined width falls within a target fraction Â± tolerance.\n",
    "    \n",
    "    Parameters:\n",
    "        flows_df (pd.DataFrame): The full flow DataFrame containing 'value' as width.\n",
    "        target_frac (float): Desired percentage of total width to remove (e.g., 0.25 for 25%)\n",
    "        tolerance (float): Percentage tolerance (e.g., 0.10 for Â±10%)\n",
    "\n",
    "    Returns:\n",
    "        list: List of selected row indices to remove.\n",
    "    \"\"\"\n",
    "    total_width = flows_df[\"value\"].sum()\n",
    "    min_width = total_width * (target_frac * (1 - tolerance))\n",
    "    max_width = total_width * (target_frac * (1 + tolerance))\n",
    "    \n",
    "    remaining_indices = list(flows_df.index)\n",
    "    random.shuffle(remaining_indices)\n",
    "\n",
    "    selected = []\n",
    "    current_sum = 0\n",
    "\n",
    "    for idx in remaining_indices:\n",
    "        flow_width = flows_df.loc[idx, \"value\"]\n",
    "        if current_sum + flow_width > max_width:\n",
    "            continue\n",
    "        selected.append(idx)\n",
    "        current_sum += flow_width\n",
    "        if current_sum >= min_width:\n",
    "            break\n",
    "\n",
    "    return selected\n",
    "\n",
    "# Example structure to confirm function works (we'll skip actually calling it here)\n",
    "select_flows_by_width_band  # ready to integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def update_results_table_with_full_metrics(\n",
    "    dataset, sankey_definition, n_trials=10, removal_fraction=0.25, X1=1, X2=1\n",
    ") -> Tuple[pd.DataFrame, dict, dict]:\n",
    "    from floweaver import Dataset, weave\n",
    "    from time import time\n",
    "\n",
    "    results = []\n",
    "    trial_details = {}\n",
    "\n",
    "    original_width = dataset._flows[\"value\"].sum()\n",
    "    flow_df_full = dataset._flows.copy()\n",
    "\n",
    "    # Compute original metrics\n",
    "    sankey_original = weave(sankey_definition, dataset)\n",
    "    sankey_orig_ordered, _, crossing_areas_orig = optimise_node_order(sankey_original, crossings__area=True)\n",
    "\n",
    "    print(\"ðŸ§ª Node layers before layout:\", sankey_orig_ordered.ordering.layers)\n",
    "\n",
    "    # ðŸ”§ FIX: Ensure minimum_gap is explicitly defined\n",
    "    layout_orig = optimise_node_positions(sankey_orig_ordered, minimum_gap=20)\n",
    "\n",
    "    y_coords_orig = {\n",
    "        nid.split(\"^\")[-1].split(\"*\")[0]: pos[1]\n",
    "        for nid, pos in layout_orig.node_positions.items()\n",
    "    }\n",
    "\n",
    "    vertical_df_orig = compute_combined_vertical_score_final(flow_df_full, y_coords_orig, sankey_definition)\n",
    "    original_vertical_total = vertical_df_orig[\"vertical_score\"].sum()\n",
    "\n",
    "    crossing_metrics_orig = compute_crossing_score_from_dict(crossing_areas_orig)\n",
    "    original_crossing_score = crossing_metrics_orig[\"crossing_score\"]\n",
    "    original_crossing_count = crossing_metrics_orig[\"crossing_count\"]\n",
    "\n",
    "    original_summary = {\n",
    "        \"original_crossings\": int(original_crossing_count),\n",
    "        \"original_crossing_score\": original_crossing_score,\n",
    "        \"original_vertical_score\": original_vertical_total,\n",
    "        \"original_total_width\": round(original_width, 2)\n",
    "    }\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        start = time()\n",
    "\n",
    "        selected_indices = select_flows_by_width_band(flow_df_full, removal_fraction)\n",
    "        width_removed = flow_df_full.loc[selected_indices, \"value\"].sum()\n",
    "        width_removed_pct = round(100 * width_removed / original_width, 2)\n",
    "\n",
    "        modified_flows = hide_flows_by_index(dataset, selected_indices)\n",
    "        new_dataset = Dataset(\n",
    "            flows=modified_flows,\n",
    "            dim_process=dataset._dim_process,\n",
    "            dim_material=dataset._dim_material,\n",
    "            dim_time=dataset._dim_time,\n",
    "        )\n",
    "\n",
    "        sankey_data = weave(sankey_definition, new_dataset)\n",
    "        sankey_data_evolved, crossings, crossing_areas = optimise_node_order(\n",
    "            sankey_data, crossings__area=True\n",
    "        )\n",
    "        ordered = optimise_node_order(sankey_data, group_nodes=True)\n",
    "\n",
    "        # ðŸ”§ FIX: Explicitly define minimum_gap\n",
    "        layout = optimise_node_positions(ordered, minimum_gap=20)\n",
    "\n",
    "        y_coords = {\n",
    "            nid.split(\"^\")[-1].split(\"*\")[0]: pos[1]\n",
    "            for nid, pos in layout.node_positions.items()\n",
    "        }\n",
    "\n",
    "        vertical_df = compute_combined_vertical_score_final(flow_df_full, y_coords, sankey_definition)\n",
    "        vertical_df[\"flow_id\"] = vertical_df.index\n",
    "        vertical_total = vertical_df[\"vertical_score\"].sum()\n",
    "\n",
    "        crossing_metrics = compute_crossing_score_from_dict(crossing_areas)\n",
    "        crossing_total = crossing_metrics[\"crossing_score\"] // 2\n",
    "        crossing_count = crossing_metrics[\"crossing_count\"] // 2\n",
    "\n",
    "        results.append({\n",
    "            \"trial\": trial + 1,\n",
    "            \"crossing_count\": crossing_count,\n",
    "            \"total_crossing_score\": crossing_total,\n",
    "            \"total_vertical_score\": vertical_total,\n",
    "            \"original_score\": vertical_total + crossing_total,\n",
    "            \"total_width\": round(original_width, 2),\n",
    "            \"width_removed\": round(width_removed, 2),\n",
    "            \"width_removed_pct\": width_removed_pct,\n",
    "            \"flow_count\": len(flow_df_full),\n",
    "            \"flows_removed\": list(selected_indices),\n",
    "        })\n",
    "\n",
    "        if trial < 3:\n",
    "            trial_details[f\"trial_{trial+1}\"] = {\n",
    "                \"y_coords\": y_coords,\n",
    "                \"vertical_table\": vertical_df[[\n",
    "                    \"flow_id\", \"source\", \"target\", \"material\", \"width\", \"span\", \"vertical_score\"\n",
    "                ]],\n",
    "                \"crossing_metrics\": crossing_metrics\n",
    "            }\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df[\"norm_vertical\"] = (results_df[\"total_vertical_score\"] - results_df[\"total_vertical_score\"].min()) / (\n",
    "        results_df[\"total_vertical_score\"].max() - results_df[\"total_vertical_score\"].min()\n",
    "    )\n",
    "    results_df[\"norm_crossing\"] = (results_df[\"total_crossing_score\"] - results_df[\"total_crossing_score\"].min()) / (\n",
    "        results_df[\"total_crossing_score\"].max() - results_df[\"total_crossing_score\"].min()\n",
    "    )\n",
    "    results_df[\"final_score\"] = X1 * results_df[\"norm_vertical\"] + X2 * results_df[\"norm_crossing\"]\n",
    "\n",
    "    summary = results_df.sort_values(\"final_score\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return summary, trial_details, original_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, logs, original_metrics = update_results_table_with_full_metrics(\n",
    "    dataset, sdd, n_trials=100, removal_fraction=0.3, X1=1, X2=1\n",
    ")\n",
    "\n",
    "# ðŸ§¾ Print original dataset metrics\n",
    "print(\"\\nðŸ“Š ORIGINAL DATASET METRICS:\")\n",
    "for key, val in original_metrics.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# ðŸ“Š Final Ranked Summary\n",
    "print(\"\\nðŸ“Š Final Ranked Summary:\")\n",
    "print(summary_df[[\n",
    "    \"trial\", \"final_score\", \"original_score\", \n",
    "    \"total_crossing_score\", \"total_vertical_score\", \n",
    "    \"crossing_count\", \"width_removed\", \"width_removed_pct\", \"flows_removed\"\n",
    "]])\n",
    "\n",
    "# ðŸ“„ Top 3 Layouts (Row Format for Easy Copying)\n",
    "print(\"\\nðŸ“„ Top 3 Layouts Summary (Row Format):\")\n",
    "top3 = summary_df.head(3)\n",
    "for i, row in top3.iterrows():\n",
    "    print(\n",
    "        f\"Trial {row['trial']} | \"\n",
    "        f\"Crossings: {row['crossing_count']} | \"\n",
    "        f\"Area: {row['total_crossing_score']} | \"\n",
    "        f\"Span: {row['total_vertical_score']} | \"\n",
    "        f\"Width: {row['total_width']} | \"\n",
    "        f\"Removed: {row['width_removed']} ({row['width_removed_pct']}%) | \"\n",
    "        f\"Flows: {row['flows_removed']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Manually specify rows to hide ---\n",
    "\n",
    "rows_to_hide =   [1, 0, 3, 13, 9]\n",
    "# --- Step 2: Hide flows in dataset ---\n",
    "def hide_flows_by_index(dataset, row_indices):\n",
    "    flows = dataset._flows.copy()\n",
    "    rows = flows.iloc[row_indices]\n",
    "\n",
    "    hidden = []\n",
    "    for _, row in rows.iterrows():\n",
    "        r1, r2 = row.copy(), row.copy()\n",
    "        r1[\"source\"] = \"HIDDEN\"\n",
    "        r2[\"target\"] = \"HIDDEN\"\n",
    "        hidden.extend([r1, r2])\n",
    "    \n",
    "    new_flows = pd.concat([\n",
    "        flows.drop(index=row_indices),\n",
    "        pd.DataFrame(hidden)\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    print(flows)\n",
    "\n",
    "    return Dataset(new_flows, dataset._dim_process, dataset._dim_material)\n",
    "\n",
    "# --- Step 3: Build new dataset with hidden flows ---\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering,flow_partition=dataset.partition('material'))\n",
    "updated_dataset = hide_flows_by_index(dataset, rows_to_hide)\n",
    "\n",
    "# --- Step 4: Create Sankey layout ---\n",
    "sankey_data = weave(sdd, updated_dataset,palette=palette)\n",
    "sankey_data_manual_evolve = optimise_node_order(sankey_data, group_nodes=True)\n",
    "\n",
    "#sankey_data_manual_evolve.to_widget(layout=optimise_node_positions(sankey_data_manual_evolve, scale = 10,minimum_gap=20, margins=dict(left=100, right=120)), width=700, height=450)   \n",
    "sankey_data_manual_evolve.to_widget(layout=optimise_node_positions(sankey_data_manual_evolve, scale = 7,minimum_gap=10, margins=dict(left=50, right=220)), width=1000, height=800)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floweaver_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
